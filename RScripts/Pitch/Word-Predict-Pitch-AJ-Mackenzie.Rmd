---
title: "Data Science Capstone:<br>Next Word Predictor"
author: "Angus Mackenzie"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  ioslides_presentation:
    widescreen: true
    css: my.css
    logo: nlp_icon.png
    smaller: yes
---


```{r setup, include=FALSE, echo=FALSE}
#### Setup
library(tidyverse);library(kableExtra);library(formattable);library(colorspace)
library(ggdist);library(ggtext);library(ggrepel)


## Functions
space <- function(x, ...) {format(x, ..., big.mark = ",", scientific = FALSE, trim = TRUE)}

spaceK <- function(x) {
  case_when(x >= 10^9 ~ paste0(format(round(x/10^9, 1), big.mark = " ", scientific = FALSE, trim = TRUE), "G"),
            x < 10^9 & x >= 10^6 ~ paste0(format(round(x/10^6, 1), big.mark = " ", scientific = FALSE, trim = TRUE), "M"),
            x < 10^6 & x >= 10^3 ~ paste0(format(round(x/10^3,1), big.mark = " ", scientific = FALSE, trim = TRUE), "K"),
            TRUE ~ as.character(space(round(x,1))))
}

my_sng_bar <- function(x, icol = NULL) {
  range <- max(abs(x), na.rm = TRUE)
  width <- c(round(abs(x / range * 100), 2))
  ifelse(is.na(x), 0, paste0(
    '<span style="display: inline-block; border-radius: 2px; ', 
    'padding-right: 0px; background-color:', icol, '; width: ', 
    width, '%; margin-left: 0%; text-align: left;">', spaceK(x), '</span>'
  ))
}

## Colours
col_bg <- "#222d32";col_fg <- "#3C8DBC";col_ct <- '#024788'
col_ml <- '#E66100';col_fc <- '#5D3A9B';col_ln <- "#748696"

## Directory
myMrk <- paste0(getwd(), "/")
    myDir <- sub("RScripts.*", "", myMrk)
        myDat <- paste0(myDir,"Data/")

## Info
dfinfo <- read.csv(paste0(myDat, "App_data_info.csv"))
vobs <- c(18534, 87595, 98481)

## Test
dftest <- read.csv(paste0(myDat, "Model_test_result.csv"))

```

##
### Predictive Text
Typing on mobile devices is hard without a full keyboard - now that Blackberry has discontinued support for its devices. To make things easier we've built an app that can do this. 

Special attention was made to keep the app small and fast while retaining accuracy.

The data used to train the model is a corpus of over four million blog, twitter and news entries. A sample of 250k entries was then cleaned, filtered and separated into 3 data frames:

```{r sampling, out.width="60%", echo=FALSE}

dfinfo %>% mutate(ngram = case_when(ngram == 1 ~ "Single words", ngram == 2 ~ "2 word combinations", 
                                    ngram == 3 ~ "3 word combinations"),
                  nmin_rep = my_sng_bar(nmin_rep, icol = col_ml),
                  nsize = my_sng_bar(nsize, icol = col_ct), nobs = vobs,
                  nobs = my_sng_bar(nobs, icol = col_fc)) %>%
  select(ngram, nmin_rep, nobs, nsize) %>%
  kable(., "html", escape = FALSE, table.attr = "style='width:100%;font-size: 16px;'", col.names = c(
    "Word Format", "Minimum Repeated", "Observations", "Size in bytes" )) %>% kable_styling(full_width = TRUE) %>%
  column_spec(1, width = 100, color = "black", extra_css = 'padding: 2px 2px 2px 2px;') %>%
  column_spec(2:4, color = "white", bold = TRUE, extra_css = 'text-align:left; padding: 2px 1px 2px 1px;') %>%
  row_spec(0, color = 'white', background = col_bg, bold = TRUE, font_size = 16,
           extra_css = 'padding: 3px 3px 3px 3px;') %>%
  row_spec(1:3, font_size = 14, extra_css = 'border-bottom-style: dashed; border-bottom-color:
           #92a8d1;border-bottom-width: thin;')


```



##
### Model Strategy 
Three different strategies were attempted:

* Simple next word prediction via n-gram tokenisation - found to be inaccurate doesn't consider the probability of unseen combinations.
* Markov chain model - great accuracy and speed, however the transition probability matrices created were far too large for use over the internet. More work will be done to remedy this problem
* Katz's back-off model - fast, accurate and what the model finally used


### Katz's back-off model
This model considers the probability of a predicted word by combining the probabilities of the word from shorter word combination predictions.

[Here is the Wikipedia entry](https://en.wikipedia.org/wiki/Katz%27s_back-off_model)

A four ngram model was considered, but size and complexity was too burdensome. 

<br>

##
### Model performance
A test on the model was made on 10 phrases checking if it could predict the word, where the actual next word was ranked by the model (number 1 is the best possible!), the probability estimated by the model and finally how long this took to process. Here are the results:

```{r performance, echo=FALSE}

vcol <- c("Successful", "Test Phrase", "Next Word", "Predicted Rank", "Predicted Probability", "Seconds Elapsed")

## Table
dftest %>% arrange(npos, desc(nprob)) %>%
  mutate(nprob = paste0(round(nprob*100, 4), "%"), ntime = round(ntime, 4)) %>%
  kable(., "html", escape = FALSE, table.attr = "style='width:100%;font-size: 16px;'", col.names = vcol) %>% 
  kable_styling(full_width = TRUE) %>%
  column_spec(1:3, color = "black", extra_css = 'padding: 2px 2px 2px 2px;') %>%
  column_spec(4:6, width = 500, color = "black", bold = TRUE, extra_css = 'text-align:center; padding: 2px 2px 2px 2px;') %>%
  row_spec(0, color = 'white', background = col_bg, bold = TRUE, font_size = 16,
           extra_css = 'padding: 3px 3px 3px 3px;') %>%
  row_spec(1:10, font_size = 14, extra_css = 'border-bottom-style: dashed; border-bottom-color:
           #92a8d1;border-bottom-width: thin;')


```

<br>

Not bad really, 5 predicted well and the two not predicted are not correctly spealt.

## 
1: Input phrase; 2: Number of predictions shown; 3: Predicted words (bigger is better!) 


```{r out.width="100%",echo=FALSE}

knitr::include_graphics(paste0(myMrk, "App_screenshot.PNG"))


```


##
### Project Repository

* The project repository can be found on Git [here](https://github.com/AngusMackenzie/Datascience_Capstone)
* The App can be accessed [here](https://angusmac-rsa.shinyapps.io/WordPredictor/)

<br>
<div class="centered">
<font size="7">
**Thank you for grading my submission!** 
</font>
<br>

</div>







